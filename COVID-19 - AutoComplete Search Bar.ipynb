{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Open Research Dataset Challenge (CORD-19)\n",
    "![](https://altaonline.typepad.com/.a/6a0192ac343706970d025d9b3673bb200c-800wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Goal</h2><br>\n",
    "    This is my first response to the call to action to the artificial intelligence experts (if I can be called one) to ddevelop text and data mining tools that can help the medical community develop answers to high priority scientific questions. For that I will use the CORD-19 dataset, which represents the most extensive machine-readable coronavirus literature collection available for data mining to date. Bellow are the current tasks for this challenge, which will be completed by the creation of a search index on top of the all_sources_metadata file, which will work independently of the metadata limitations. There are around 29500 papers in the dataset. These are listed in the all_sources_metadata file. Some of the papers in the metadata are also in JSON files. The eventual goal is to connect the metadata with the JSON data.<br>\n",
    "    <h2>Tasks</h2>\n",
    "    <ul>\n",
    "    <li>What is known about transmission, incubation, and environmental stability?</li>\n",
    "    <li>What do we know about COVID-19 risk factors?</li>\n",
    "    <li>What do we know about virus genetics, origin, and evolution?</li>\n",
    "    <li>Sample task with sample submission</li>\n",
    "    <li>What do we know about vaccines and therapeutics?</li>\n",
    "    <li>What do we know about non-pharmaceutical interventions?</li>\n",
    "    <li>What has been published about ethical and social science considerations?</li>\n",
    "    <li>What do we know about diagnostics and surveillance?</li>\n",
    "    <li>What has been published about medical care?</li>\n",
    "    <li>What has been published about information sharing and inter-sectoral collaboration?</li>\n",
    "    </ul>\n",
    "    <h2>Citations, ups and downs</h2><br>\n",
    "    I used the <a href='https://www.kaggle.com/dgunning/browsing-research-papers-with-a-bm25-search-engine'>Browsing research papers with a BM25 search engine</a> as a reference for making this notebook. I was looking forward to testing different NLP solutions and the one presented here was very interesting. On the plus side, it offers a very dynamic online research index for all the papers, with a return that looks beautiful. On the down side, it needs you to have your internet working, and depending on the size of the dataframe, it can take a while to load (mine took a few minutes). Besides that I am still working on a way to split authors when they are separated by comma or by semicolon.<br>\n",
    "    <h2>Features of this notebook</h2>\n",
    "<ol><li>Viewing the papers in the metdata csv as a dataframe</li>\n",
    "    <li>Selecting individual papers</li>\n",
    "    <li>Search using a simple search index using RankBM25</li>\n",
    "    <li>Autocomplete search bar</li></ol>\n",
    "    <h2>Turn your internet on!</h2><br>\n",
    "    For this notebook to work your internet must be on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank_bm25 in c:\\programdata\\anaconda3\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from rank_bm25) (1.16.5)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Collecting ipywidgets==7.1.1\n",
      "  Using cached ipywidgets-7.1.1-py2.py3-none-any.whl (68 kB)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets==7.1.1) (7.8.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets==7.1.1) (4.4.0)\n",
      "Collecting widgetsnbextension~=3.1.0\n",
      "  Using cached widgetsnbextension-3.1.4-py2.py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets==7.1.1) (5.1.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets==7.1.1) (4.3.3)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.1.1) (0.4.1)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.1.1) (0.15.1)\n",
      "Requirement already satisfied: pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.1.1) (2.4.2)\n",
      "Requirement already satisfied: pickleshare in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.1.1) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.1.1) (0.1.0)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.1.1) (4.4.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\beatriz yumi\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.1.1) (46.0.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.1.1) (2.0.10)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets==7.1.1) (3.0.2)\n",
      "Requirement already satisfied: ipython-genutils in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets==7.1.1) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets==7.1.1) (4.5.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (6.0.1)\n",
      "Requirement already satisfied: jupyter-client in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets==7.1.1) (5.3.3)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets==7.1.1) (6.0.3)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from traitlets>=4.3.1->ipywidgets==7.1.1) (1.12.0)\n",
      "Requirement already satisfied: parso>=0.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.1.1) (0.5.1)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.1.1) (0.1.7)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.1.1) (0.15.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.1.1) (19.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (2.10.3)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (0.8.2)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (18.1.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (0.7.1)\n",
      "Requirement already satisfied: Send2Trash in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (1.5.0)\n",
      "Requirement already satisfied: nbconvert in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (5.6.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets==7.1.1) (223)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets==7.1.1) (2.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (1.1.1)\n",
      "Requirement already satisfied: bleach in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (3.1.0)\n",
      "Requirement already satisfied: testpath in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (0.4.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (0.6.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (1.4.2)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.1.0->ipywidgets==7.1.1) (0.5.1)\n",
      "Installing collected packages: widgetsnbextension, ipywidgets\n",
      "Successfully installed ipywidgets-7.1.1 widgetsnbextension-3.1.4\n"
     ]
    }
   ],
   "source": [
    "# installing the libraries needed (in my case they were already installed)\n",
    "\n",
    "!pip install rank_bm25 nltk\n",
    "!pip install --user ipywidgets==7.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Beatriz\n",
      "[nltk_data]     Yumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Beatriz\n",
      "[nltk_data]     Yumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import HTTPError, ConnectionError\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "import re\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e257c275cd224833942feaba9d3a14b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=200, description='ColumnWidth', max=400, min=50, step=50), IntSlider(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adjusting the size of columns and rows\n",
    "# I left the rows to 29500 because I want all the results to be shown\n",
    "\n",
    "def set_column_width(ColumnWidth, MaxRows):\n",
    "    pd.options.display.max_colwidth = ColumnWidth\n",
    "    pd.options.display.max_rows = MaxRows\n",
    "    print('Set pandas dataframe column width to', ColumnWidth, 'and max rows to', MaxRows)\n",
    "    \n",
    "interact(set_column_width, \n",
    "         ColumnWidth=widgets.IntSlider(min=50, max=400, step=50, value=200),\n",
    "         MaxRows=widgets.IntSlider(min=50, max=29500, step=100, value=29500));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>c630ebcdf30652f0422c3ec12a00b50241dc9bd9</td>\n",
       "      <td>CZI</td>\n",
       "      <td>Angiotensin-converting enzyme 2 (ACE2) as a SARS-CoV-2 receptor: molecular mechanisms and potential therapeutic target</td>\n",
       "      <td>10.1007/s00134-020-05985-9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32125455</td>\n",
       "      <td>cc-by-nc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "      <td>Zhang, Haibo; Penninger, Josef M.; Li, Yimin; Zhong, Nanshan; Slutsky, Arthur S.</td>\n",
       "      <td>Intensive Care Med</td>\n",
       "      <td>2002765492</td>\n",
       "      <td>#3252</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>53eccda7977a31e3d0f565c884da036b1e85438e</td>\n",
       "      <td>CZI</td>\n",
       "      <td>Comparative genetic analysis of the novel coronavirus (2019-nCoV/SARS-CoV-2) receptor ACE2 in different populations</td>\n",
       "      <td>10.1038/s41421-020-0147-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "      <td>Cao, Yanan; Li, Lin; Feng, Zhimin; Wan, Shengqing; Huang, Peide; Sun, Xiaohui; Wen, Fang; Huang, Xuanlin; Ning, Guang; Wang, Weiqing</td>\n",
       "      <td>Cell Discovery</td>\n",
       "      <td>3003430844</td>\n",
       "      <td>#1861</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>210a892deb1c61577f6fba58505fd65356ce6636</td>\n",
       "      <td>CZI</td>\n",
       "      <td>Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data</td>\n",
       "      <td>10.3390/jcm9020538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The geographic spread of 2019 novel coronavirus (COVID-19) infections from the epicenter of Wuhan, China, has provided an opportunity to study the natural history of the recently emerged virus. Us...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Linton, M. Natalie; Kobayashi, Tetsuro; Yang, Yichi; Hayashi, Katsuma; Akhmetzhanov, R. Andrei; Jung, Sung-mok; Yuan, Baoyin; Kinoshita, Ryo; Nishiura, Hiroshi</td>\n",
       "      <td>Journal of Clinical Medicine</td>\n",
       "      <td>3006065484</td>\n",
       "      <td>#1043</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>e3b40cc8e0e137c416b4a2273a4dca94ae8178cc</td>\n",
       "      <td>CZI</td>\n",
       "      <td>Characteristics of and Public Health Responses to the Coronavirus Disease 2019 Outbreak in China</td>\n",
       "      <td>10.3390/jcm9020575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32093211</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>In December 2019, cases of unidentified pneumonia with a history of exposure in the Huanan Seafood Market were reported in Wuhan, Hubei Province. A novel coronavirus, SARS-CoV-2, was identified to...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Deng, Sheng-Qun; Peng, Hong-Juan</td>\n",
       "      <td>J Clin Med</td>\n",
       "      <td>177663115</td>\n",
       "      <td>#1999</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>92c2c9839304b4f2bc1276d41b1aa885d8b364fd</td>\n",
       "      <td>CZI</td>\n",
       "      <td>Imaging changes in severe COVID-19 pneumonia</td>\n",
       "      <td>10.1007/s00134-020-05976-w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32125453</td>\n",
       "      <td>cc-by-nc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "      <td>Zhang, Wei</td>\n",
       "      <td>Intensive Care Med</td>\n",
       "      <td>3006643024</td>\n",
       "      <td>#3242</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sha source_x  \\\n",
       "0  c630ebcdf30652f0422c3ec12a00b50241dc9bd9      CZI   \n",
       "1  53eccda7977a31e3d0f565c884da036b1e85438e      CZI   \n",
       "2  210a892deb1c61577f6fba58505fd65356ce6636      CZI   \n",
       "3  e3b40cc8e0e137c416b4a2273a4dca94ae8178cc      CZI   \n",
       "4  92c2c9839304b4f2bc1276d41b1aa885d8b364fd      CZI   \n",
       "\n",
       "                                                                                                                                                                            title  \\\n",
       "0                                                          Angiotensin-converting enzyme 2 (ACE2) as a SARS-CoV-2 receptor: molecular mechanisms and potential therapeutic target   \n",
       "1                                                             Comparative genetic analysis of the novel coronavirus (2019-nCoV/SARS-CoV-2) receptor ACE2 in different populations   \n",
       "2  Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data   \n",
       "3                                                                                Characteristics of and Public Health Responses to the Coronavirus Disease 2019 Outbreak in China   \n",
       "4                                                                                                                                    Imaging changes in severe COVID-19 pneumonia   \n",
       "\n",
       "                          doi pmcid pubmed_id   license  \\\n",
       "0  10.1007/s00134-020-05985-9   NaN  32125455  cc-by-nc   \n",
       "1   10.1038/s41421-020-0147-1   NaN       NaN     cc-by   \n",
       "2          10.3390/jcm9020538   NaN       NaN     cc-by   \n",
       "3          10.3390/jcm9020575   NaN  32093211     cc-by   \n",
       "4  10.1007/s00134-020-05976-w   NaN  32125453  cc-by-nc   \n",
       "\n",
       "                                                                                                                                                                                                  abstract  \\\n",
       "0                                                                                                                                                                                                      NaN   \n",
       "1                                                                                                                                                                                                      NaN   \n",
       "2  The geographic spread of 2019 novel coronavirus (COVID-19) infections from the epicenter of Wuhan, China, has provided an opportunity to study the natural history of the recently emerged virus. Us...   \n",
       "3  In December 2019, cases of unidentified pneumonia with a history of exposure in the Huanan Seafood Market were reported in Wuhan, Hubei Province. A novel coronavirus, SARS-CoV-2, was identified to...   \n",
       "4                                                                                                                                                                                                      NaN   \n",
       "\n",
       "  publish_time  \\\n",
       "0         2020   \n",
       "1         2020   \n",
       "2         2020   \n",
       "3         2020   \n",
       "4         2020   \n",
       "\n",
       "                                                                                                                                                           authors  \\\n",
       "0                                                                                 Zhang, Haibo; Penninger, Josef M.; Li, Yimin; Zhong, Nanshan; Slutsky, Arthur S.   \n",
       "1                             Cao, Yanan; Li, Lin; Feng, Zhimin; Wan, Shengqing; Huang, Peide; Sun, Xiaohui; Wen, Fang; Huang, Xuanlin; Ning, Guang; Wang, Weiqing   \n",
       "2  Linton, M. Natalie; Kobayashi, Tetsuro; Yang, Yichi; Hayashi, Katsuma; Akhmetzhanov, R. Andrei; Jung, Sung-mok; Yuan, Baoyin; Kinoshita, Ryo; Nishiura, Hiroshi   \n",
       "3                                                                                                                                 Deng, Sheng-Qun; Peng, Hong-Juan   \n",
       "4                                                                                                                                                       Zhang, Wei   \n",
       "\n",
       "                        journal Microsoft Academic Paper ID WHO #Covidence  \\\n",
       "0            Intensive Care Med                  2002765492          #3252   \n",
       "1                Cell Discovery                  3003430844          #1861   \n",
       "2  Journal of Clinical Medicine                  3006065484          #1043   \n",
       "3                    J Clin Med                   177663115          #1999   \n",
       "4            Intensive Care Med                  3006643024          #3242   \n",
       "\n",
       "  has_full_text  \n",
       "0          True  \n",
       "1          True  \n",
       "2          True  \n",
       "3          True  \n",
       "4         False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import metadata\n",
    "\n",
    "metadata_path = 'metadata.csv'\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str\n",
    "})\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Treating the dataset</h2><br>\n",
    "Besides treating all the dataset, we will create means, functions and utilities to work with the data and have an autofilling search index and a dropdown menu for the tasks in kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media2.giphy.com/media/l2YWmdzX0DkNHk3ew/giphy.gif?cid=790b76114fe3a6219b32e6f19c686f11f629724e736bcc5e&rid=giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the doi to a url\n",
    "\n",
    "def doi_url(d): return f'http://{d}' if d.startswith('doi.org') else f'http://doi.org/{d}'\n",
    "meta_df.doi = meta_df.doi.fillna('').apply(doi_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the abstract to the paper title if it is null\n",
    "\n",
    "meta_df.abstract = meta_df.abstract.fillna(meta_df.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many entries there are in the DataFrame\n",
    "\n",
    "len(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicated papers\n",
    "\n",
    "duplicate_paper = ~(meta_df.title.isnull() | meta_df.abstract.isnull()) & (meta_df.duplicated(subset=['title', 'abstract']))\n",
    "meta_df = meta_df[~duplicate_paper].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25133"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many entries there are in the DataFrame\n",
    "\n",
    "len(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to get the requests from an url\n",
    "\n",
    "def get(url, timeout=6):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        return r.text\n",
    "    except ConnectionError:\n",
    "        print(f'Cannot connect to {url}')\n",
    "        print(f'Remember to turn Internet ON in the Kaggle notebook settings')\n",
    "    except HTTPError:\n",
    "        print('Got http error', r.status, r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a wrapper for a DataFrame with useful functions for notebooks\n",
    "\n",
    "class DataHolder:    \n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data        \n",
    "        \n",
    "    def __len__(self): return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item): return self.data.loc[item]\n",
    "    \n",
    "    def head(self, n:int): return DataHolder(self.data.head(n).copy())\n",
    "    \n",
    "    def tail(self, n:int): return DataHolder(self.data.tail(n).copy())\n",
    "    \n",
    "    def _repr_html_(self): return self.data._repr_html_()\n",
    "    \n",
    "    def __repr__(self): return self.data.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a wrapper for the entire dataset and provides useful functions to navigate through it\n",
    "\n",
    "class ResearchPapers:\n",
    "    \n",
    "    def __init__(self, metadata: pd.DataFrame):\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return Paper(self.metadata.iloc[item])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def head(self, n):\n",
    "        return ResearchPapers(self.metadata.head(n).copy().reset_index(drop=True))\n",
    "    \n",
    "    def tail(self, n):\n",
    "        return ResearchPapers(self.metadata.tail(n).copy().reset_index(drop=True))\n",
    "    \n",
    "    def abstracts(self):\n",
    "        return self.metadata.abstract.dropna()\n",
    "    \n",
    "    def titles(self):\n",
    "        return self.metadata.title.dropna()\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.metadata._repr_html_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a wrapper for each research paper\n",
    "\n",
    "class Paper:\n",
    "    \n",
    "    # single research paper \n",
    "    \n",
    "    def __init__(self, item):\n",
    "        self.paper = item.to_frame().fillna('')\n",
    "        self.paper.columns = ['Value']\n",
    "    \n",
    "    def doi(self):\n",
    "        return self.paper.loc['doi'].values[0]\n",
    "    \n",
    "    def html(self):\n",
    "        \n",
    "        # loads the paper from doi.org and displays it as HTML. Needs internet on\n",
    "        \n",
    "        text = get(self.doi())\n",
    "        return widgets.HTML(text)\n",
    "    \n",
    "    def text(self):\n",
    "        \n",
    "        # loads the paper from doi.org and display as text. Needs internet on\n",
    "        \n",
    "        text = get(self.doi())\n",
    "        return text\n",
    "    \n",
    "    def abstract(self):\n",
    "        return self.paper.loc['abstract'].values[0]\n",
    "    \n",
    "    def title(self):\n",
    "        return self.paper.loc['title'].values[0]\n",
    "    \n",
    "    def authors(self, split=False):\n",
    "        \n",
    "        # gets a list of authors\n",
    "        \n",
    "        authors = self.paper.loc['authors'].values[0]\n",
    "        if not authors:\n",
    "            return []\n",
    "        if not split:\n",
    "            return authors\n",
    "        if authors.startswith('['):\n",
    "            authors = authors.lstrip('[').rstrip(']')\n",
    "            return [a.strip().replace(\"\\'\", \"\") for a in authors.split(\"\\',\")]\n",
    "        \n",
    "        # Todo: Handle cases where author names are separated by \",\"\n",
    "        return [a.strip() for a in authors.split(';')]\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.paper._repr_html_()\n",
    "    \n",
    "\n",
    "papers = ResearchPapers(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = papers.head(29500).abstracts()\n",
    "type(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of stopwords in english\n",
    "\n",
    "english_stopwords = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that cleans text of special characters\n",
    "\n",
    "def strip_characters(text):\n",
    "    t = re.sub('\\(|\\)|:|,|;|\\.|’|”|“|\\?|%|>|<', '', text)\n",
    "    t = re.sub('/', ' ', t)\n",
    "    t = t.replace(\"'\",'')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that makes text lowercase and uses the function created above\n",
    "\n",
    "def clean(text):\n",
    "    t = text.lower()\n",
    "    t = strip_characters(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize into individual tokens - words mostly\n",
    "\n",
    "def tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return list(set([word for word in words \n",
    "                     if len(word) > 1\n",
    "                     and not word in english_stopwords\n",
    "                     and not (word.isnumeric() and len(word) is not 4)\n",
    "                     and (not word.isnumeric() or word.isalpha())] )\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that cleans and tokenize texts\n",
    "\n",
    "def preprocess(text):\n",
    "    t = clean(text)\n",
    "    tokens = tokenize(t)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a wrapper for the search results\n",
    "\n",
    "class SearchResults:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame,\n",
    "                 columns = None):\n",
    "        self.results = data\n",
    "        if columns:\n",
    "            self.results = self.results[columns]\n",
    "            \n",
    "    def __getitem__(self, item):\n",
    "        return Paper(self.results.loc[item])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.results)\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.results._repr_html_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining column names of the search display\n",
    "\n",
    "SEARCH_DISPLAY_COLUMNS = ['title', 'abstract', 'doi', 'authors', 'journal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a wrapper for the word tokens which will be searched\n",
    "\n",
    "class WordTokenIndex:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 corpus: pd.DataFrame, \n",
    "                 columns=SEARCH_DISPLAY_COLUMNS):\n",
    "        self.corpus = corpus\n",
    "        raw_search_str = self.corpus.abstract.fillna('') + ' ' + self.corpus.title.fillna('')\n",
    "        self.index = raw_search_str.apply(preprocess).to_frame()\n",
    "        self.index.columns = ['terms']\n",
    "        self.index.index = self.corpus.index\n",
    "        self.columns = columns\n",
    "    \n",
    "    def search(self, search_string):\n",
    "        search_terms = preprocess(search_string)\n",
    "        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n",
    "        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n",
    "        return SearchResults(results, self.columns + ['paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the search index class\n",
    "\n",
    "class RankBM25Index(WordTokenIndex):\n",
    "    \n",
    "    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n",
    "        super().__init__(corpus, columns)\n",
    "        self.bm25 = BM25Okapi(self.index.terms.tolist())\n",
    "        \n",
    "    def search(self, search_string, n=4):\n",
    "        search_terms = preprocess(search_string)\n",
    "        doc_scores = self.bm25.get_scores(search_terms)\n",
    "        ind = np.argsort(doc_scores)[::-1][:n]\n",
    "        results = self.corpus.iloc[ind][self.columns]\n",
    "        results['Score'] = doc_scores[ind]\n",
    "        results = results[results.Score > 0]\n",
    "        return SearchResults(results.reset_index(), self.columns + ['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the search index\n",
    "\n",
    "bm25_index = RankBM25Index(meta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating a dropdown menu for the kaggle tasks</h2><br>\n",
    "In this dropdown menu you can select the task and it will return all the papers related to the keywords associated with that task. The keywords were created based on the specific requirement of each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media3.giphy.com/media/3o7TKMlJrVQ5ket3hu/giphy.gif?cid=790b76116b55efb8ce288d0f1c58ee050f5810df1d6368af&rid=giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list for the tasks in the challenge and it's keywords\n",
    "\n",
    "tasks = [('What is known about transmission, incubation, and environmental stability?', 'transmission periods asymptomatic shedding persistance stability substrate surfaces diagnostics disease models tools immune immunity effectiveness strategy movement health care community PPE seasonality incubation environmental stability coronavirus'),\n",
    "         ('What do we know about COVID-19 risk factors?', 'COVID-19 risk factors coronavirus smoking pre-existing pulmonary disease neonates pregnant women severity transmission dynamics susceptibility of populations public health mitigation measures that could be effective for control'),\n",
    "         ('What do we know about virus genetics, origin, and evolution?', 'virus genetics origin evolution coronavirus real-time tracking genome dissemination diagnostics geographic temporal genomic strain nagoya protocol livestock field surveillance genetic sequencing receptor binding farmers wildlife SARS-CoV-2 animal host socioeconomic behavioral reduction risk'),\n",
    "         ('Sample task with sample submission', 'sample submission geographic variation variations mutation evidence'),\n",
    "         ('What do we know about vaccines and therapeutics?', 'drugs clinical bench trials less common viral inhabitors naproxen clarithromycin minocyclinethat Antibody-Dependent Enhancement (ADE) in vaccine recipients animal model predictive value therapeutic alternative model prioritize distribution expanding production universal vaccine standardize prophylaxis clinical studies enhanced disease immune response'),\n",
    "         ('What do we know about non-pharmaceutical interventions?', 'guidance scale up NPI funding infrastructure authorities support authoritative collaboration health care delivery system capacity respond increase case design execution experiment DHS center excelence assessment school closure travel ban mass gathering social distancing spread communities barriers compliance intervention cost benefit race income disability age geographic location immigration status housing employment health insurance compliance underserved advice pandemicpolicy programmatic mitigate government service food distribuition supplies food household diagnose treatment'),\n",
    "         ('What has been published about ethical and social science considerations?', 'effort articulate translate existing ethical principle standard salient issues COVID-19 thematic novel duplicate oversight sustained education access capacity WHO multidisciplinary research operational platform global network social science qualitative assessment framework local barrier enabler adherence public health measure prevention control surgical mask SRH school closure outbreak physical physiological underlying driver fear anxiety stigma misinformation rumor social media'),\n",
    "         ('What do we know about diagnostics and surveillance?', 'widespread exposure immediate policy recommentation mitigation measure denominator testing mechanism sharing information demographics sampling asymptomatic disease serosurvey convalescent screening neutralizing antibodies ELISA diagnostic surveillance recruitment legal ethical communication public health official national guidance guidelines best practices state universities private laboratories tradeoff speed accuracy accessibility PCR specific entity assay private sector evolution genetic drift mutation reagent latency pathogen cytokines progression therapeutical policies roadmap coalition epidemic preparedness inovation CRISPR genomics rapid sequencing bioinformatic wildlife domestic risk'),\n",
    "         ('What has been published about medical care?', 'nursing long term care medical staff shortage overwhelmed communities age-ajusted mortality Acute Respiratory Distress Syndrome ARDS organ failure viral etiology Extracorporeal membrane oxygenation ECMO mechanical ventilation frequency manifestation cardiomyopathy cardiac arrest regulatory standard EUA CLIA crisis care level elastomeric respirator N95 mask telemedicine barriers facilitators specific action state boundaries guidance oral medication AI real-time health care delivery valuate interventions risk factors outcomes best practices critical challenges innovative solutions technologies flow organization workforce protection allocation community-based resources payment supply chain natural history interventions steroids high flow oxygen'),\n",
    "         ('What has been published about information sharing and inter-sectoral collaboration?', 'data-gathering standardized nomenclature sharing response information with planners providers mitigating barriers of information-sharing recruit support coordinate local expertise capacity public health emergency response integration federal state local public health surveillance systems investment baseline infrastructure preparedness high-risk population elderly health care workers guidelines easy understand risk disease population group misunderstanding containment mitigation action plan surveillance treatment marginalized disadvantaged data system incarcareted COVID-19 information prevention diagnosis coverage policies')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the list into a DataFrame\n",
    "\n",
    "tasks = pd.DataFrame(tasks, columns=['Task', 'Keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3309b5726ca49e1b223afdabbe30ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Task', options=('What is known about transmission, incubation, and…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a dropdown menu for each task\n",
    "\n",
    "def show_task(Task):\n",
    "    print(Task)\n",
    "    keywords = tasks[tasks.Task == Task].Keywords.values[0]\n",
    "    search_results = bm25_index.search(keywords, n=29500)\n",
    "    display(search_results)\n",
    "    return search_results\n",
    "    \n",
    "results = interact(show_task, Task = tasks.Task.tolist());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating an autocomplete search bar</h2><br>\n",
    "Why go through the trouble of typing everything when this search engine can do everything for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media1.giphy.com/media/Ebys71phzQQx2/giphy.gif?cid=790b7611a3c7952b8ed20b09249ca2cd3774beef22dd1560&rid=giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses IPywidgets interactive rendering of a TextBox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating autocomplete search bar\n",
    "\n",
    "def search_papers(SearchTerms: str):\n",
    "    search_results = bm25_index.search(SearchTerms, n=29500)\n",
    "    if len(search_results) > 0:\n",
    "        display(search_results) \n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6983f6c137b491987a9cd8f7d85305d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='SearchTerms'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Autocomplete search bar\n",
    "\n",
    "searchbar = widgets.interactive(search_papers, SearchTerms='')\n",
    "searchbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
